#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Workflow å‘½ä»¤è¡Œå·¥å…·
ç”¨äºæ‰§è¡Œå„ç§æ•°æ®å¤„ç†å·¥ä½œæµ
"""

import argparse
import sys
import os
from pathlib import Path

# å°†é¡¹ç›®æ ¹ç›®å½•æ·»åŠ åˆ° Python è·¯å¾„
project_root = os.path.dirname(os.path.dirname(__file__))
if project_root not in sys.path:
    sys.path.insert(0, project_root)

from src.store.sqlite_conn import get_sqlite_db
from src.store.sqlite_repo import ChapterChunkRepo
from src.chapter_chunk_extractor_fanren_impl import ChapterChunkExtractor
from src.models import ChapterChunk


def process_chapter_chunks(file_path: str, encoding: str):
    """
    å¤„ç†ç« èŠ‚åˆ†å—å·¥ä½œæµï¼šåŠ è½½æ–‡æ¡£ -> åˆ†å— -> å­˜å‚¨åˆ° SQLite

    Args:
        file_path: æ–‡æ¡£æ–‡ä»¶è·¯å¾„
        encoding: æ–‡ä»¶ç¼–ç æ ¼å¼
    """
    try:
        print(f"ğŸ“ å¼€å§‹å¤„ç†æ–‡ä»¶: {file_path}")
        print(f"ğŸ”¤ ç¼–ç æ ¼å¼: {encoding}")

        # 1. æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        full_path = os.path.join(project_root, file_path)
        if not os.path.exists(full_path):
            print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {full_path}")
            return

        # 2. åŠ è½½æ–‡æ¡£å†…å®¹
        print(f"ğŸ“– æ­£åœ¨åŠ è½½æ–‡æ¡£...")
        with open(full_path, 'r', encoding=encoding) as f:
            content = f.read()

        print(f"âœ… æ–‡æ¡£åŠ è½½æˆåŠŸï¼Œæ€»å­—ç¬¦æ•°: {len(content):,}")

        # 3. æ‰§è¡Œç« èŠ‚åˆ†å—
        print(f"ğŸ”§ å¼€å§‹ç« èŠ‚åˆ†å—å¤„ç†...")
        extractor = ChapterChunkExtractor()

        # éœ€è¦æä¾›å°è¯´åç§°ï¼Œä»æ–‡ä»¶åæ¨æ–­
        novel_name = "fanren"  # å¯ä»¥æ ¹æ®éœ€è¦ä¿®æ”¹
        chunks = extractor.extract_chapter_chunks(novel_name, content)
        print(f"âœ… åˆ†å—å®Œæˆï¼Œå…±ç”Ÿæˆ {len(chunks)} ä¸ªç« èŠ‚å—")

        # 4. æ‰¹é‡å­˜å‚¨åˆ° SQLite æ•°æ®åº“
        print(f"ğŸ’¾ å¼€å§‹æ‰¹é‡å­˜å‚¨åˆ° SQLite æ•°æ®åº“...")

        with get_sqlite_db() as db:
            conn = db.get_connection()

            # ä½¿ç”¨æ‰¹é‡ UPSERT æ“ä½œå­˜å‚¨æ‰€æœ‰ç« èŠ‚å—
            processed_count = ChapterChunkRepo.upsert_chunks(conn, chunks)

            # æäº¤äº‹åŠ¡
            conn.commit()

        print(f"âœ… æ‰¹é‡å­˜å‚¨å®Œæˆï¼")
        print(f"ğŸ“Š å¤„ç†ç»Ÿè®¡:")
        print(f"   - æ€»ç« èŠ‚æ•°: {len(chunks)}")
        print(f"   - æˆåŠŸå¤„ç†: {processed_count}")
        print(f"   - æ€»å­—ç¬¦æ•°: {sum(chunk.char_count for chunk in chunks):,}")
        print(f"   - æ€»Tokenæ•°: {sum(chunk.token_count for chunk in chunks):,}")

    except Exception as e:
        print(f"âŒ å¤„ç†å¤±è´¥: {e}")
        sys.exit(1)


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description="Workflow å·¥ä½œæµå·¥å…·",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  python workflow_cli.py -m chapter_chunk -f resources/ignored/1.txt gb18030
  python workflow_cli.py -m chapter_chunk -f data/novel.txt utf-8
        """
    )

    # å·¥ä½œæµæ¨¡å—å‚æ•°
    parser.add_argument(
        '-m', '--module',
        required=True,
        choices=['chapter_chunk'],
        help='é€‰æ‹©è¦æ‰§è¡Œçš„å·¥ä½œæµæ¨¡å—'
    )

    # æ–‡ä»¶å‚æ•°
    parser.add_argument(
        '-f', '--file',
        required=True,
        help='è¾“å…¥æ–‡ä»¶è·¯å¾„ï¼ˆç›¸å¯¹äºé¡¹ç›®æ ¹ç›®å½•ï¼‰'
    )

    # ç¼–ç å‚æ•°
    parser.add_argument(
        'encoding',
        help='æ–‡ä»¶ç¼–ç æ ¼å¼ï¼ˆå¦‚ï¼šgb18030, utf-8, gbkç­‰ï¼‰'
    )

    args = parser.parse_args()

    if args.module == 'chapter_chunk':
        process_chapter_chunks(args.file, args.encoding)
    else:
        print(f"âŒ æœªçŸ¥çš„å·¥ä½œæµæ¨¡å—: {args.module}")
        sys.exit(1)


if __name__ == '__main__':
    main()